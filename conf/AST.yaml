# Env config
seed: 147
GPUs: "0" # "0,1,2,3"

# In
train_csv: !PLACEHOLDER
test_csv: !PLACEHOLDER

# Out
exp_name: !PLACEHOLDER
exp_dir: !ref exp/<exp_name>-<seed>

# Feat extractor: [m, num_frames, num_mels]
sampling_rate: 48000
max_duration: 5.0 # sec
feat_extractor: !new:transformers.ASTFeatureExtractor
    sampling_rate: !ref <sampling_rate>
    padding: "max_length"
    return_tensors: "pt"
    return_attention_mask: True
    feature_size: 1
    num_mel_bins: 128
    padding_side: "right"
    padding_value: !!float 0.0
    max_length: 1024
    do_normalize: True
    mean: !!float -4.569388389587402
    std: !!float 5.2317891120910645
    # Note: tune the mean, std, max_length values in local/ast-get_norm_stats.py
    #       avg_mean = 0.0
    #       avg_std = 0.5
    #       max_length covers num_frames

# AST model
num_classes: 4
ast_model_fpath: src/AST/ast-model
finetuned_layers: null

# Training Arguments
num_workers: 32
batch_size: 16
num_epoch: 30
learning_rate: 1e-5
model_ckpts: !ref <exp_dir>/ckpts
training_args: !new:transformers.TrainingArguments
    output_dir: !ref <exp_dir>
    evaluation_strategy: !name:transformers.IntervalStrategy.STEPS
    save_strategy: "steps"
    save_steps: !!int 50  # Save checkpoints after ... steps
    eval_steps: !!int 50  # Evaluation takes places every ... steps
    save_total_limit: 5   # Only last ... models are saved. Older ones are deleted.
    logging_steps: !!int 50
    learning_rate: !ref <learning_rate>
    per_device_train_batch_size: !ref <batch_size>
    per_device_eval_batch_size: !ref <batch_size>
    dataloader_num_workers: !ref <num_workers>
    gradient_accumulation_steps: 4
    eval_accumulation_steps: 4
    num_train_epochs: !ref <num_epoch>
    load_best_model_at_end: True
    warmup_ratio: 0.1
    metric_for_best_model: "f1"
    seed: !ref <seed>
    push_to_hub: False

early_stopping: !new:transformers.EarlyStoppingCallback
    early_stopping_patience: 20
    # early_stopping_threshold: 0.5

# Trainer config
# compute_cost: !new:torch.nn.NLLLoss
compute_cost: !new:torch.nn.CrossEntropyLoss
